{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ARiQzHgDuix"
      },
      "source": [
        "# **AI in Medicine**\n",
        "\n",
        "Welcome to the second  programming workshop for the AI in Radiology!\n",
        "\n",
        "Today we will be diving into some more advanced concepts in machine learning:\n",
        "*    Supervised learning: Decision Trees and understanding Overfitting and Knn Classifiers\n",
        "\n",
        "\n",
        "Before you begin reading and editing any code, make sure to make a personal copy of this notebook by clicking `File` --> `Save a Copy in Drive` so you can make changes to the code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HgpqzZZCLxg"
      },
      "source": [
        "## Imports and loading the dataset\n",
        "Let's start the necessary `import`s for our code. We will go to the breast cancer dataset for a supervised learning topics.\n",
        "\n",
        "**Confused about something?** Raise questions during the session! Also remember that you can always Google a function or piece of code that you're not sure about and you will find lots of documentation explaining what is happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVaBIZqLCt86"
      },
      "source": [
        "# Load the necessary python libraries\n",
        "from sklearn import preprocessing, decomposition\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
        "# deprecated from sklearn.metrics import plot_confusion_matrix, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import graphviz\n",
        "from graphviz import Source\n",
        "from IPython.display import SVG\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use('bmh')\n",
        "plt.rcParams.update({'font.size': 14,\n",
        "                     'xtick.labelsize' : 14,\n",
        "                     'ytick.labelsize' : 14,\n",
        "                     'figure.figsize' : [12,8],\n",
        "                     })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJXjeUpFasz"
      },
      "source": [
        "# **Supervised Learning**: Decision Trees for Classification\n",
        "\n",
        "This session we will look at another supervised learning technique, Decision Trees. Recall that last session we looked at logistic regression, which is suitable for classifying data that is *linearly separable*. Decision trees, on the other hand, can learn non-linear classification boundaries which is suitable for more complex datasets. Another benefit of decision trees is that they are **easy to interpret**! This is because, for a trained decision tree classifier, you can directly view the decisions that are being made in the algorithm which lead to the final classification. As a result, decision trees are a great option for medical problems (such as reaching a diagnosis based on certain patient symptoms) where it is important to explain and understand how an algorithm reaches its decision.\n",
        "\n",
        "Once again, we will be analyzing the breast cancer dataset. We will use decision trees to learn how to predict benign from malignant tumors, and we will investigate how the accuracy changes as we change the depth of the decision tree.\n",
        "\n",
        "**Reminder:** the breast cancer dataset has 539 tumor cases with 30 features each. Feel free to visualize and plot the features to explore the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpJsGPVBJE2c",
        "outputId": "42d382c0-c696-431b-a39d-026e75c5f0fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load breast cancer dataset and store as a Pandas dataframe. This code is the same as used in the previous session\n",
        "cancer = load_breast_cancer()\n",
        "scaler = preprocessing.StandardScaler().fit(cancer.data) # Define a scaler which standardizes the data to have mean 0 and standard devation 1. This usually improves stability and performance\n",
        "cancer_data_scaled = scaler.transform(cancer.data) #Use the scaler to transform the training and test data\n",
        "\n",
        "# get statistics\n",
        "# print the number of samples with cancer\n",
        "\n",
        "n_samples_cancer = (cancer.target == 0).sum()\n",
        "print(n_samples_cancer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k79nIp_MNBP"
      },
      "source": [
        "## Training a decision tree and understanding overfitting:\n",
        "\n",
        "`sklearn` provides all the code necessary to create and train a decision tree.\n",
        "\n",
        "One of the most important parameters that we need to settle on for a decision tree is its maximum depth, which is essentially how many levels of decisions we want the algorithm to make before arriving at the final classification. If we pick a decision tree with too few levels (e.g. `max_depth = 1`), then it will not be complex enough to accurately differentiate between benign and malignant tumors. On the other hand, if our decision tree has too many levels (e.g. `max_depth = 15` for our dataset), then the algorithm will try to perfectly classify the training set over 15 levels of questions. The issue here is that this perfect classification will not generalize to the validation set, and so the validation accuracy will be poor. We will do a simple experiment to figure out how to pick the appropriate decision tree depth, and you can use this type of analysis in the future to settle on other important parameters for algorithms.\n",
        "\n",
        "This will be our experiment:\n",
        "1.  Split the dataset into training and validation sets with a split of 75%/25%\n",
        "2.  Train 15 different decision tree classifiers with exactly the same parameters, with the exception of maximum depth which varies from 1 to 15.\n",
        "3.  Visualize training and validation set accuracy for each decision tree\n",
        "4.  Visualize the decision tree with the most appropriate maximum depth\n",
        "5. Visualize the best decision tree results as a confusion matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dP3Wg_MHGLg"
      },
      "source": [
        "# This is an important hyperparameter which determines how the training and validation data is split.\n",
        "# Try changing it to another integer and rerunning the max_depth experiment below.\n",
        "random_state = 13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MwBNHnPctZt"
      },
      "source": [
        "# Split the dataset into training and validation sets\n",
        "X_train, X_validation, y_train, y_validation = train_test_split(cancer_data_scaled, cancer.target, random_state=random_state)\n",
        "\n",
        "# Create empty variables to store the decision tree accuracy results\n",
        "training_accuracy = []\n",
        "validation_accuracy = []\n",
        "\n",
        "# Define the range of decision tree as 1 - 15\n",
        "max_dep = range(1,16)\n",
        "\n",
        "# Use a for loop to try each of the maximum depth decision trees, and store the scores in the appropriate lists\n",
        "for md in max_dep:\n",
        "    tree = DecisionTreeClassifier(max_depth=md,random_state=0)\n",
        "    tree.fit(X_train,y_train)\n",
        "    training_accuracy.append(tree.score(X_train, y_train))\n",
        "    validation_accuracy.append(tree.score(X_validation, y_validation))\n",
        "\n",
        "# Plot the tree depths against training and validation set accuracies\n",
        "plt.figure()\n",
        "plt.plot(max_dep,training_accuracy, label='Training set')\n",
        "plt.plot(max_dep,validation_accuracy, label='Validation set')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpH9JFC-Gkw6"
      },
      "source": [
        "## What is the optimal tree depth?\n",
        "## Can you explain why the training set accuracy keeps increasing with depth?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta9OrEZ3eGYW"
      },
      "source": [
        "# set the oprtimal depth as new max_depth\n",
        "#### YOUR CODE HERE ####\n",
        "max_depth = ???\n",
        "\n",
        "\n",
        "# now we can fir a new DecisionTreeClassifier with our new max_depth and evaluate it\n",
        "tree = DecisionTreeClassifier(max_depth=max_depth,random_state=0)\n",
        "# TODO: Train the classifier, e.g. fit it to the data  X_train and y_train\n",
        "#### YOUR CODE HERE ####\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "training_accuracy = tree.score(X_train, y_train)\n",
        "validation_accuracy = tree.score(X_validation, y_validation)\n",
        "\n",
        "\n",
        "# TODO: print the accuracy for the training and the validation set\n",
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Visualize decision tree\n",
        "# TODO: Run the code to print the most important feature\n",
        "graph = Source(export_graphviz(tree, out_file=None, class_names=['malignant','benign'], feature_names=cancer.feature_names, impurity=False, filled=True))\n",
        "SVG(graph.pipe(format='svg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run the code print the most important feature\n",
        "MOST_IMPORTANT = ...\n",
        "print(MOST_IMPORTANT)"
      ],
      "metadata": {
        "id": "HibMqrgp_Kla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BONUS**: Can you improve the accuary by building a knn Classifier?\n",
        "\n",
        "1. Check https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html on how to build a knn Classifier\n",
        "2. Loop over a range of k to find the best performing one\n",
        "3. Store the results in a loop and then plot them\n",
        "4. Train an knn-classifier with the best k and compare the validation_accuracy for the decision tree and the knn-Classifier"
      ],
      "metadata": {
        "id": "EeR1PLt8E3mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the range of k\n",
        "#### YOUR CODE HERE ####\n",
        "from_k =...\n",
        "to_k = ...\n",
        "\n",
        "# lists to store the results\n",
        "train_accuracies_knn = []\n",
        "validation_accuracies_knn = []\n",
        "\n",
        "# Loop over the k and train for each a knn Classifier\n",
        "for i in range(from_k, to_k):\n",
        "  #### YOUR CODE HERE ####"
      ],
      "metadata": {
        "id": "NazuuGVL_3U8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the results and select best k\n",
        "#### YOUR CODE HERE ####"
      ],
      "metadata": {
        "id": "mKQWclJXHytc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train knn-Classifier with best k and compute the accuracy for the training set and the validation set\n",
        "#### YOUR CODE HERE ####\n",
        "best_k = ...\n",
        "\n",
        "# fit knn-Classifier\n",
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "#### YOUR CODE HERE ####\n",
        "knn_train_accuracy = ...\n",
        "knn_validation_accuracy = ...\n",
        "\n",
        "\n",
        "# compare the accucarcies of both models\n",
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Z7bkX6MABdp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do you think? Why does the knn Classifier perform better here?**"
      ],
      "metadata": {
        "id": "G0MxSJ5iOddD"
      }
    }
  ]
}
